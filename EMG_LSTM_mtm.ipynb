{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "EMG_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mzAgNivdaAw",
        "colab_type": "code",
        "outputId": "295b7cde-b4ef-4816-9b63-8cb949fe564e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhnnGGq8kyWI",
        "colab_type": "text"
      },
      "source": [
        "###数据导入和处理\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOnuqxz-daA1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import genfromtxt\n",
        "import numpy as np\n",
        "my_data1 = genfromtxt('EMG_1ac10sec_MAT1-12.csv', delimiter=',')\n",
        "my_data2 = genfromtxt('EMG_1ac10sec_MAT3-12.csv', delimiter=',')\n",
        "my_data3 = genfromtxt('EMG_1ac10sec_MAT4-12.csv', delimiter=',')\n",
        "my_data4 = genfromtxt('EMG_1ac10sec_MAO4-12.csv', delimiter=',')\n",
        "my_data5 = genfromtxt('EMG_1ac10sec_MAO5-12.csv', delimiter=',')\n",
        "my_data = np.concatenate((my_data1,my_data2,my_data3,my_data4,my_data5),axis=1)\n",
        "# my_data=my_data1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YySGxvzUD1-l",
        "colab_type": "text"
      },
      "source": [
        "#####每stepsize个连续的原始数据组成一个时间子序列，允许每两个label边界处两个label划分到一个子序列的情况(该方法的好处是：将多个数据集concatenate到一起后更容易做切片操作),然后将每个子序列(子序列长度为stepsize)作为新的example输入到 LSTM 中。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRO5GkEk1p93",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stepsize=30\n",
        "\n",
        "data = my_data[1:10]\n",
        "mydata = data.T\n",
        "len_d=mydata.shape[0]     #length of data\n",
        "num_s=int(len_d/stepsize) #number of subsequence\n",
        "data=np.ndarray((num_s,stepsize,9))\n",
        "for i in range(num_s):\n",
        "  for j in range(stepsize):\n",
        "    for k in range(9):\n",
        "      data[i,j,k]=mydata[stepsize*i+j,k]  #slice every stepsize original examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCsdMsvsdaBC",
        "colab_type": "text"
      },
      "source": [
        "Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjR7Os79P-qs",
        "colab_type": "code",
        "outputId": "f299fb6a-adfc-48b4-c8b9-a54c6e06a558",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#分割数据时需要考虑是否 shuffle 数据, here we shuffle the data\n",
        "np.random.seed(0)\n",
        "np.random.shuffle(data)\n",
        "features=data[:,:,0:8]\n",
        "label=data[:,:,8:9]-1\n",
        "label=np.reshape(label,(label.shape[0]*label.shape[1],1))\n",
        "print(\"original shape: {0}\".format(features.shape))\n",
        "\n",
        "length =features.shape[0]\n",
        "train_x = features[0:int(length*0.7), :,:]\n",
        "train_y = label[0:int(length*0.7)*stepsize, :]\n",
        "dev_x = features[int(length*0.7):int(length*0.9),:,:]\n",
        "dev_y = label[int(length*0.7)*stepsize:int(length*0.9)*stepsize, :]\n",
        "test_x=features[int(length*0.9):,:,:]\n",
        "test_y=label[int(length*0.9)*stepsize:,:]\n",
        "print(\"splitted shape:{0}\" .format(dev_y.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original shape: (7139, 27, 8)\n",
            "splitted shape:(38556, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdS3sOe5daBV",
        "colab_type": "text"
      },
      "source": [
        "label的格式要和model要求的格式吻合，但是onehot之后的是二维数据，需要用reshape扩维;\n",
        "Onehot的使用方法：使用sklearn库的LabelBinarizer方法进行 fit_transform,前提是数据本身得是 2 维的，如果维数不合适可在fit_transform之后使用numpy.reshape方法进行维数变换"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpNckhXOnKRi",
        "colab_type": "text"
      },
      "source": [
        "Label Binarizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_alVnFUodaBW",
        "colab_type": "code",
        "outputId": "7a043b20-a97e-4b1f-fab9-c405e18b9bff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "label_encoder=LabelBinarizer()\n",
        "y_train=label_encoder.fit_transform(train_y)\n",
        "y_val=label_encoder.fit_transform(dev_y)\n",
        "y_test=label_encoder.fit_transform(test_y)\n",
        "y_train=np.reshape(y_train,(int(y_train.shape[0]/stepsize),stepsize,y_train.shape[1]))\n",
        "y_val=np.reshape(y_val,(int(y_val.shape[0]/stepsize),stepsize,y_val.shape[1]))\n",
        "y_test=np.reshape(y_test,(int(y_test.shape[0]/stepsize),stepsize,y_test.shape[1]))\n",
        "print(\"onehot之后的shape:{0}\".format(y_test.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "onehot之后的shape:(714, 27, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9V4IIVFdaBc",
        "colab_type": "text"
      },
      "source": [
        "## Building network\n",
        "\n",
        "For this reason we will use larger layers. Let's go with 64 units:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru--jiYSdaBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "model = models.Sequential()\n",
        "model.add(layers.LSTM(64,input_shape=(stepsize,8),activation='relu',return_sequences=True,dropout=0))\n",
        "model.add(layers.LSTM(64, activation='relu',return_sequences=True))\n",
        "model.add(layers.LSTM(32, activation='relu',return_sequences=True,dropout=0))\n",
        "model.add(layers.LSTM(16, activation='relu',return_sequences=True,dropout=0))\n",
        "model.add(layers.Dense(12, activation='softmax'))\n",
        "epoch = 100\n",
        "learning_rate = 0.001\n",
        "decay_rate = learning_rate / epoch\n",
        "momentum = 0.8\n",
        "adam = optimizers.Adam(lr=learning_rate, decay=decay_rate)\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usJm-Bby2Tpv",
        "colab_type": "text"
      },
      "source": [
        "## Set checkpoints to save the best model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYkW3W3E12Li",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import os\n",
        "import argparse\n",
        "# ap = argparse.ArgumentParser()\n",
        "# ap.add_argument(\"-w\",\"--weights\",required=True, help=\"path to weights directory\")\n",
        "# args = vars(ap.parse_args())\n",
        "fname = \"weights.best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(fname,monitor=\"val_acc\",mode=\"max\", save_best_only=True,verbose=1)\n",
        "callbacks = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ltwcvw8wdaBi",
        "colab_type": "text"
      },
      "source": [
        "## Validating our approach\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey5mWLtkdaBj",
        "colab_type": "text"
      },
      "source": [
        "Now let's train our network using grid search:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDZFCfeFdaBk",
        "colab_type": "code",
        "outputId": "3060bae0-7514-43e0-b383-e91473c3eb01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(train_x, y_train, epochs=epoch, \n",
        "                    batch_size=500, validation_data=(dev_x, y_val),callbacks=callbacks)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4997 samples, validate on 1428 samples\n",
            "Epoch 1/100\n",
            "4997/4997 [==============================] - 21s 4ms/step - loss: 2.2953 - acc: 0.1517 - val_loss: 2.1197 - val_acc: 0.1830\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.18295, saving model to weights.best.hdf5\n",
            "Epoch 2/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 1.9340 - acc: 0.2352 - val_loss: 1.7404 - val_acc: 0.3244\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.18295 to 0.32444, saving model to weights.best.hdf5\n",
            "Epoch 3/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 1.5439 - acc: 0.3748 - val_loss: 1.5178 - val_acc: 0.3513\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.32444 to 0.35128, saving model to weights.best.hdf5\n",
            "Epoch 4/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 1.3000 - acc: 0.4696 - val_loss: 1.2559 - val_acc: 0.4821\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.35128 to 0.48208, saving model to weights.best.hdf5\n",
            "Epoch 5/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 1.1743 - acc: 0.5232 - val_loss: 1.2842 - val_acc: 0.4719\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.48208\n",
            "Epoch 6/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 1.0927 - acc: 0.5559 - val_loss: 1.0430 - val_acc: 0.5696\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.48208 to 0.56964, saving model to weights.best.hdf5\n",
            "Epoch 7/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.9972 - acc: 0.5957 - val_loss: 1.0142 - val_acc: 0.6180\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.56964 to 0.61801, saving model to weights.best.hdf5\n",
            "Epoch 8/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.9241 - acc: 0.6355 - val_loss: 0.9007 - val_acc: 0.6542\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.61801 to 0.65419, saving model to weights.best.hdf5\n",
            "Epoch 9/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.9084 - acc: 0.6541 - val_loss: 0.8810 - val_acc: 0.6810\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.65419 to 0.68101, saving model to weights.best.hdf5\n",
            "Epoch 10/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.8801 - acc: 0.6617 - val_loss: 0.8263 - val_acc: 0.6801\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.68101\n",
            "Epoch 11/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.7959 - acc: 0.6927 - val_loss: 0.7895 - val_acc: 0.6988\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.68101 to 0.69880, saving model to weights.best.hdf5\n",
            "Epoch 12/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.7035 - acc: 0.7371 - val_loss: 0.7512 - val_acc: 0.7230\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.69880 to 0.72300, saving model to weights.best.hdf5\n",
            "Epoch 13/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.6660 - acc: 0.7505 - val_loss: 0.6751 - val_acc: 0.7614\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.72300 to 0.76144, saving model to weights.best.hdf5\n",
            "Epoch 14/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.6242 - acc: 0.7700 - val_loss: 0.6257 - val_acc: 0.7761\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.76144 to 0.77612, saving model to weights.best.hdf5\n",
            "Epoch 15/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.5788 - acc: 0.7883 - val_loss: 0.5931 - val_acc: 0.7840\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.77612 to 0.78403, saving model to weights.best.hdf5\n",
            "Epoch 16/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.5439 - acc: 0.8027 - val_loss: 0.5556 - val_acc: 0.8050\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.78403 to 0.80501, saving model to weights.best.hdf5\n",
            "Epoch 17/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.5351 - acc: 0.8057 - val_loss: 0.5067 - val_acc: 0.8213\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.80501 to 0.82127, saving model to weights.best.hdf5\n",
            "Epoch 18/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.4980 - acc: 0.8209 - val_loss: 0.5123 - val_acc: 0.8208\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.82127\n",
            "Epoch 19/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.4698 - acc: 0.8330 - val_loss: 0.5127 - val_acc: 0.8221\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.82127 to 0.82208, saving model to weights.best.hdf5\n",
            "Epoch 20/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.4620 - acc: 0.8345 - val_loss: 0.4650 - val_acc: 0.8378\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.82208 to 0.83779, saving model to weights.best.hdf5\n",
            "Epoch 21/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.4381 - acc: 0.8436 - val_loss: 0.4659 - val_acc: 0.8355\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.83779\n",
            "Epoch 22/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.4361 - acc: 0.8442 - val_loss: 0.4853 - val_acc: 0.8267\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.83779\n",
            "Epoch 23/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.4123 - acc: 0.8526 - val_loss: 0.4566 - val_acc: 0.8416\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.83779 to 0.84158, saving model to weights.best.hdf5\n",
            "Epoch 24/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.3943 - acc: 0.8580 - val_loss: 0.4327 - val_acc: 0.8520\n",
            "\n",
            "Epoch 00024: val_acc improved from 0.84158 to 0.85198, saving model to weights.best.hdf5\n",
            "Epoch 25/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.3916 - acc: 0.8613 - val_loss: 0.4315 - val_acc: 0.8462\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.85198\n",
            "Epoch 26/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.3772 - acc: 0.8661 - val_loss: 0.4041 - val_acc: 0.8577\n",
            "\n",
            "Epoch 00026: val_acc improved from 0.85198 to 0.85769, saving model to weights.best.hdf5\n",
            "Epoch 27/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.3718 - acc: 0.8669 - val_loss: 0.3971 - val_acc: 0.8625\n",
            "\n",
            "Epoch 00027: val_acc improved from 0.85769 to 0.86246, saving model to weights.best.hdf5\n",
            "Epoch 28/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.4559 - acc: 0.8452 - val_loss: 0.4426 - val_acc: 0.8512\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.86246\n",
            "Epoch 29/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.3793 - acc: 0.8676 - val_loss: 0.3937 - val_acc: 0.8623\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.86246\n",
            "Epoch 30/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.3475 - acc: 0.8746 - val_loss: 0.4143 - val_acc: 0.8553\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.86246\n",
            "Epoch 31/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.3460 - acc: 0.8772 - val_loss: 0.3648 - val_acc: 0.8713\n",
            "\n",
            "Epoch 00031: val_acc improved from 0.86246 to 0.87128, saving model to weights.best.hdf5\n",
            "Epoch 32/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.3272 - acc: 0.8815 - val_loss: 0.3954 - val_acc: 0.8619\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.87128\n",
            "Epoch 33/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.3169 - acc: 0.8864 - val_loss: 0.3824 - val_acc: 0.8645\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.87128\n",
            "Epoch 34/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.3190 - acc: 0.8858 - val_loss: 0.3468 - val_acc: 0.8784\n",
            "\n",
            "Epoch 00034: val_acc improved from 0.87128 to 0.87836, saving model to weights.best.hdf5\n",
            "Epoch 35/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.3061 - acc: 0.8899 - val_loss: 0.3533 - val_acc: 0.8755\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.87836\n",
            "Epoch 36/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.3008 - acc: 0.8906 - val_loss: 0.3449 - val_acc: 0.8804\n",
            "\n",
            "Epoch 00036: val_acc improved from 0.87836 to 0.88036, saving model to weights.best.hdf5\n",
            "Epoch 37/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2973 - acc: 0.8934 - val_loss: 0.3515 - val_acc: 0.8777\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.88036\n",
            "Epoch 38/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2963 - acc: 0.8945 - val_loss: 0.3180 - val_acc: 0.8904\n",
            "\n",
            "Epoch 00038: val_acc improved from 0.88036 to 0.89042, saving model to weights.best.hdf5\n",
            "Epoch 39/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2927 - acc: 0.8951 - val_loss: 0.3669 - val_acc: 0.8756\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.89042\n",
            "Epoch 40/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2780 - acc: 0.9000 - val_loss: 0.3217 - val_acc: 0.8873\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.89042\n",
            "Epoch 41/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2795 - acc: 0.9000 - val_loss: 0.3550 - val_acc: 0.8765\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.89042\n",
            "Epoch 42/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2667 - acc: 0.9049 - val_loss: 0.3681 - val_acc: 0.8769\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.89042\n",
            "Epoch 43/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2710 - acc: 0.9021 - val_loss: 0.3394 - val_acc: 0.8791\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.89042\n",
            "Epoch 44/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2666 - acc: 0.9037 - val_loss: 0.3215 - val_acc: 0.8903\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.89042\n",
            "Epoch 45/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2564 - acc: 0.9073 - val_loss: 0.3505 - val_acc: 0.8828\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.89042\n",
            "Epoch 46/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2607 - acc: 0.9074 - val_loss: 0.3146 - val_acc: 0.8914\n",
            "\n",
            "Epoch 00046: val_acc improved from 0.89042 to 0.89140, saving model to weights.best.hdf5\n",
            "Epoch 47/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2489 - acc: 0.9106 - val_loss: 0.2927 - val_acc: 0.9010\n",
            "\n",
            "Epoch 00047: val_acc improved from 0.89140 to 0.90100, saving model to weights.best.hdf5\n",
            "Epoch 48/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2523 - acc: 0.9082 - val_loss: 0.3127 - val_acc: 0.8904\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.90100\n",
            "Epoch 49/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2581 - acc: 0.9088 - val_loss: 0.3186 - val_acc: 0.8913\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.90100\n",
            "Epoch 50/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.8579 - acc: 0.7619 - val_loss: 0.5533 - val_acc: 0.7924\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.90100\n",
            "Epoch 51/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.4566 - acc: 0.8289 - val_loss: 0.4825 - val_acc: 0.8203\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.90100\n",
            "Epoch 52/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.3819 - acc: 0.8548 - val_loss: 0.3974 - val_acc: 0.8547\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.90100\n",
            "Epoch 53/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.3409 - acc: 0.8729 - val_loss: 0.3838 - val_acc: 0.8626\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.90100\n",
            "Epoch 54/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.3199 - acc: 0.8815 - val_loss: 0.3703 - val_acc: 0.8692\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.90100\n",
            "Epoch 55/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.3045 - acc: 0.8876 - val_loss: 0.3753 - val_acc: 0.8641\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.90100\n",
            "Epoch 56/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2929 - acc: 0.8921 - val_loss: 0.3392 - val_acc: 0.8776\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.90100\n",
            "Epoch 57/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2796 - acc: 0.8971 - val_loss: 0.3434 - val_acc: 0.8778\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.90100\n",
            "Epoch 58/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2709 - acc: 0.9015 - val_loss: 0.3425 - val_acc: 0.8813\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.90100\n",
            "Epoch 59/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2665 - acc: 0.9024 - val_loss: 0.3199 - val_acc: 0.8857\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.90100\n",
            "Epoch 60/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2619 - acc: 0.9053 - val_loss: 0.3376 - val_acc: 0.8792\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.90100\n",
            "Epoch 61/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2526 - acc: 0.9088 - val_loss: 0.3172 - val_acc: 0.8897\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.90100\n",
            "Epoch 62/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2421 - acc: 0.9111 - val_loss: 0.3290 - val_acc: 0.8866\n",
            "\n",
            "Epoch 00062: val_acc did not improve from 0.90100\n",
            "Epoch 63/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2426 - acc: 0.9119 - val_loss: 0.3245 - val_acc: 0.8884\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.90100\n",
            "Epoch 64/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2718 - acc: 0.9047 - val_loss: 0.3132 - val_acc: 0.8882\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.90100\n",
            "Epoch 65/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2420 - acc: 0.9115 - val_loss: 0.3187 - val_acc: 0.8894\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.90100\n",
            "Epoch 66/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2308 - acc: 0.9157 - val_loss: 0.3194 - val_acc: 0.8899\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.90100\n",
            "Epoch 67/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2203 - acc: 0.9203 - val_loss: 0.3037 - val_acc: 0.8941\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.90100\n",
            "Epoch 68/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2219 - acc: 0.9197 - val_loss: 0.3354 - val_acc: 0.8884\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 0.90100\n",
            "Epoch 69/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2210 - acc: 0.9194 - val_loss: 0.2964 - val_acc: 0.8982\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 0.90100\n",
            "Epoch 70/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2196 - acc: 0.9204 - val_loss: 0.3189 - val_acc: 0.8911\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.90100\n",
            "Epoch 71/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2146 - acc: 0.9218 - val_loss: 0.3402 - val_acc: 0.8916\n",
            "\n",
            "Epoch 00071: val_acc did not improve from 0.90100\n",
            "Epoch 72/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2090 - acc: 0.9234 - val_loss: 0.2993 - val_acc: 0.8998\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 0.90100\n",
            "Epoch 73/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2143 - acc: 0.9216 - val_loss: 0.3073 - val_acc: 0.8940\n",
            "\n",
            "Epoch 00073: val_acc did not improve from 0.90100\n",
            "Epoch 74/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2142 - acc: 0.9226 - val_loss: 0.2829 - val_acc: 0.9002\n",
            "\n",
            "Epoch 00074: val_acc did not improve from 0.90100\n",
            "Epoch 75/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2072 - acc: 0.9249 - val_loss: 0.3031 - val_acc: 0.8988\n",
            "\n",
            "Epoch 00075: val_acc did not improve from 0.90100\n",
            "Epoch 76/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2046 - acc: 0.9256 - val_loss: 0.3021 - val_acc: 0.9000\n",
            "\n",
            "Epoch 00076: val_acc did not improve from 0.90100\n",
            "Epoch 77/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2069 - acc: 0.9252 - val_loss: 0.2827 - val_acc: 0.9039\n",
            "\n",
            "Epoch 00077: val_acc improved from 0.90100 to 0.90393, saving model to weights.best.hdf5\n",
            "Epoch 78/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.1945 - acc: 0.9289 - val_loss: 0.2846 - val_acc: 0.9031\n",
            "\n",
            "Epoch 00078: val_acc did not improve from 0.90393\n",
            "Epoch 79/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.1917 - acc: 0.9302 - val_loss: 0.2863 - val_acc: 0.9043\n",
            "\n",
            "Epoch 00079: val_acc improved from 0.90393 to 0.90432, saving model to weights.best.hdf5\n",
            "Epoch 80/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.1968 - acc: 0.9286 - val_loss: 0.2844 - val_acc: 0.9033\n",
            "\n",
            "Epoch 00080: val_acc did not improve from 0.90432\n",
            "Epoch 81/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.1889 - acc: 0.9310 - val_loss: 0.3248 - val_acc: 0.8896\n",
            "\n",
            "Epoch 00081: val_acc did not improve from 0.90432\n",
            "Epoch 82/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2345 - acc: 0.9155 - val_loss: 0.3439 - val_acc: 0.8801\n",
            "\n",
            "Epoch 00082: val_acc did not improve from 0.90432\n",
            "Epoch 83/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2885 - acc: 0.8981 - val_loss: 0.3089 - val_acc: 0.8930\n",
            "\n",
            "Epoch 00083: val_acc did not improve from 0.90432\n",
            "Epoch 84/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2159 - acc: 0.9222 - val_loss: 0.3154 - val_acc: 0.8933\n",
            "\n",
            "Epoch 00084: val_acc did not improve from 0.90432\n",
            "Epoch 85/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2016 - acc: 0.9270 - val_loss: 0.2930 - val_acc: 0.9005\n",
            "\n",
            "Epoch 00085: val_acc did not improve from 0.90432\n",
            "Epoch 86/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.1884 - acc: 0.9325 - val_loss: 0.2803 - val_acc: 0.9050\n",
            "\n",
            "Epoch 00086: val_acc improved from 0.90432 to 0.90505, saving model to weights.best.hdf5\n",
            "Epoch 87/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2018 - acc: 0.9267 - val_loss: 0.2774 - val_acc: 0.9043\n",
            "\n",
            "Epoch 00087: val_acc did not improve from 0.90505\n",
            "Epoch 88/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.1858 - acc: 0.9322 - val_loss: 0.2867 - val_acc: 0.9037\n",
            "\n",
            "Epoch 00088: val_acc did not improve from 0.90505\n",
            "Epoch 89/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.1849 - acc: 0.9326 - val_loss: 0.3155 - val_acc: 0.8960\n",
            "\n",
            "Epoch 00089: val_acc did not improve from 0.90505\n",
            "Epoch 90/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.1938 - acc: 0.9300 - val_loss: 0.4132 - val_acc: 0.8596\n",
            "\n",
            "Epoch 00090: val_acc did not improve from 0.90505\n",
            "Epoch 91/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.2107 - acc: 0.9245 - val_loss: 0.2828 - val_acc: 0.9037\n",
            "\n",
            "Epoch 00091: val_acc did not improve from 0.90505\n",
            "Epoch 92/100\n",
            "4997/4997 [==============================] - 15s 3ms/step - loss: 0.1813 - acc: 0.9354 - val_loss: 0.2728 - val_acc: 0.9066\n",
            "\n",
            "Epoch 00092: val_acc improved from 0.90505 to 0.90663, saving model to weights.best.hdf5\n",
            "Epoch 93/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.1720 - acc: 0.9386 - val_loss: 0.2904 - val_acc: 0.9041\n",
            "\n",
            "Epoch 00093: val_acc did not improve from 0.90663\n",
            "Epoch 94/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.1789 - acc: 0.9355 - val_loss: 0.3069 - val_acc: 0.8984\n",
            "\n",
            "Epoch 00094: val_acc did not improve from 0.90663\n",
            "Epoch 95/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.1812 - acc: 0.9345 - val_loss: 0.2920 - val_acc: 0.9027\n",
            "\n",
            "Epoch 00095: val_acc did not improve from 0.90663\n",
            "Epoch 96/100\n",
            "4997/4997 [==============================] - 15s 3ms/step - loss: 0.1767 - acc: 0.9354 - val_loss: 0.2797 - val_acc: 0.9038\n",
            "\n",
            "Epoch 00096: val_acc did not improve from 0.90663\n",
            "Epoch 97/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.1707 - acc: 0.9385 - val_loss: 0.2877 - val_acc: 0.9042\n",
            "\n",
            "Epoch 00097: val_acc did not improve from 0.90663\n",
            "Epoch 98/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.1836 - acc: 0.9341 - val_loss: 0.2655 - val_acc: 0.9117\n",
            "\n",
            "Epoch 00098: val_acc improved from 0.90663 to 0.91171, saving model to weights.best.hdf5\n",
            "Epoch 99/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.1600 - acc: 0.9423 - val_loss: 0.2938 - val_acc: 0.9063\n",
            "\n",
            "Epoch 00099: val_acc did not improve from 0.91171\n",
            "Epoch 100/100\n",
            "4997/4997 [==============================] - 14s 3ms/step - loss: 0.1723 - acc: 0.9375 - val_loss: 0.2729 - val_acc: 0.9097\n",
            "\n",
            "Epoch 00100: val_acc did not improve from 0.91171\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ykb00s64daBm",
        "colab_type": "text"
      },
      "source": [
        "Let's display its loss and accuracy curves:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tnCWRdMdaBn",
        "colab_type": "code",
        "outputId": "a436100e-3792-49f3-bd3c-3eaf02cfe0ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMMYYn/v/1Ua/phIDgHkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "QPVXfH0bdaBq",
        "colab_type": "code",
        "outputId": "554bfce2-37c7-4439-eb68-e76121916bf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.clf()   # clear figure\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9o2NjMydaBs",
        "colab_type": "text"
      },
      "source": [
        "## TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx3c5Irk0EsH",
        "colab_type": "code",
        "outputId": "2b1b37a7-3a42-42ea-c83c-de262b9ca4ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19278, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyDVc0UUdaBt",
        "colab_type": "code",
        "outputId": "c00a50e4-d971-4655-8cb4-82c4732286e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "source": [
        "#test_x=np.reshape(test_x,(test_x.shape[0],1,test_x.shape[1]))\n",
        "#predictions = model.predict(test_x)\n",
        "...\n",
        "# evaluate the keras model\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "model.load_weights('weights.best.hdf5')\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "_, accuracy = model.evaluate(test_x, y_test)\n",
        "print('Stepsize: %d \\nAccuracy: %.2f' % (stepsize,accuracy*100))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-daac61efefda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m               \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m               metrics=['accuracy'])\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Stepsize: %d \\nAccuracy: %.2f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstepsize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1286\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1289\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    793\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    129\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    132\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_7 to have 3 dimensions, but got array with shape (19278, 12)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCW3OjthvGeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test = np.reshape(y_test,(y_test.shape[0]*y_test.shape[1],y_test.shape[2]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6u-na27daBv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(test_x)\n",
        "print(y_test.shape)\n",
        "y_pred = np.reshape(y_pred,(y_pred.shape[0]*y_pred.shape[1],y_pred.shape[2]))\n",
        "\n",
        "# accuracy = accuracy_score(y_test,y_pred.round())\n",
        "matrix = confusion_matrix(y_test.argmax(axis=1),y_pred.argmax(axis=1))\n",
        "# print('Accuracy: %.2f' % (accuracy*100))\n",
        "print('Confusion Matrix:\\n%s'%(matrix))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVTmfpCBuzRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}